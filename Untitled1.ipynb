{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e06c87db-f038-4ea3-9e43-b527614d371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels loaded: {0: '1', 1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'J', 11: 'K', 12: 'L', 13: 'M', 14: 'N', 15: 'O', 16: 'P', 17: 'Q', 18: 'R', 19: 'space'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- Load the Model and Class Map ---\n",
    "try:\n",
    "    with open('model.p', 'rb') as f:\n",
    "        load_data = pickle.load(f)\n",
    "        model = load_data['model']\n",
    "        class_map = load_data['class_map']\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: model.p not found. Please run the training script first.\")\n",
    "    exit()\n",
    "\n",
    "# Create a dictionary to map class indices to class names\n",
    "labels_dict = {v: k for k, v in class_map.items()}\n",
    "print(f\"Labels loaded: {labels_dict}\")\n",
    "\n",
    "# --- Initialize Webcam and MediaPipe ---\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "# --- Variables for Sentence Logic ---\n",
    "sentence = \"\"\n",
    "last_prediction = None\n",
    "prediction_stable_for = 0\n",
    "last_char_time = time.time()\n",
    "PREDICTION_THRESHOLD = 20  # Frames the gesture must be stable\n",
    "COOLDOWN_PERIOD = 2      # Seconds to wait after adding a character\n",
    "\n",
    "# --- Main Loop ---\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame.\")\n",
    "        break\n",
    "    \n",
    "    H, W, _ = frame.shape\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    \n",
    "    # Create a white box at the bottom for the sentence\n",
    "    cv2.rectangle(frame, (0, H - 60), (W, H), (255, 255, 255), -1)\n",
    "    \n",
    "    # --- Hand Landmark Processing ---\n",
    "    if results.multi_hand_landmarks:\n",
    "        all_landmarks = []\n",
    "        for hand_landmarks in results.multi_hand_landmarks[:2]: # Process up to 2 hands\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "            data_aux = []\n",
    "            x_ = [lm.x for lm in hand_landmarks.landmark]\n",
    "            y_ = [lm.y for lm in hand_landmarks.landmark]\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                data_aux.append(hand_landmarks.landmark[i].x - min(x_))\n",
    "                data_aux.append(hand_landmarks.landmark[i].y - min(y_))\n",
    "            all_landmarks.extend(data_aux)\n",
    "\n",
    "        # Pad with zeros if only one hand is detected\n",
    "        if len(all_landmarks) < 84:\n",
    "            all_landmarks.extend([0] * (84 - len(all_landmarks)))\n",
    "\n",
    "        # --- Prediction and Logic for Space ---\n",
    "        prediction = model.predict([np.asarray(all_landmarks[:84])])\n",
    "        predicted_class_name = labels_dict[int(prediction[0])]\n",
    "\n",
    "        # Display the gesture name the model sees\n",
    "        cv2.putText(frame, f'Gesture: {predicted_class_name}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "        \n",
    "        # This is the key part: Map the word \"space\" to a real space character \" \"\n",
    "        if predicted_class_name == \"space\":\n",
    "            current_char = \" \"\n",
    "        else:\n",
    "            current_char = predicted_class_name\n",
    "        \n",
    "        # --- Stability and Cooldown Logic ---\n",
    "        if current_char == last_prediction:\n",
    "            prediction_stable_for += 1\n",
    "        else:\n",
    "            prediction_stable_for = 1\n",
    "            last_prediction = current_char\n",
    "            \n",
    "        if prediction_stable_for >= PREDICTION_THRESHOLD and (time.time() - last_char_time) > COOLDOWN_PERIOD:\n",
    "            sentence += current_char\n",
    "            last_char_time = time.time()\n",
    "            prediction_stable_for = 0\n",
    "            \n",
    "    # Display the final sentence\n",
    "    cv2.putText(frame, sentence, (20, H - 20), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Sign Language Recognition', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# --- Cleanup ---\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe3d920-cc8e-4c19-b2bc-875659585308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping created: {'1': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'space': 19}\n",
      "Error loading file .ipynb_checkpoints: [Errno 13] Permission denied: './data\\\\M\\\\.ipynb_checkpoints'\n",
      "Training model...\n",
      "Training complete.\n",
      "99.86% accuracy achieved on the test set!\n",
      "Model and class map saved as model.p\n"
     ]
    }
   ],
   "source": [
    "# 2_train_model_named.py\n",
    "# MODIFIED: Reads named folders and saves a class map with the model.\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = './data'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# MODIFIED: Create a mapping from folder name (word) to a number (label)\n",
    "# Sort the directory names to ensure consistent mapping\n",
    "class_names = sorted(os.listdir(DATA_DIR))\n",
    "class_map = {name: i for i, name in enumerate(class_names)}\n",
    "print(f\"Class mapping created: {class_map}\")\n",
    "\n",
    "# Load the data and assign numeric labels based on the folder name\n",
    "for class_name, class_label in class_map.items():\n",
    "    class_dir = os.path.join(DATA_DIR, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    \n",
    "    for file_path in os.listdir(class_dir):\n",
    "        try:\n",
    "            with open(os.path.join(class_dir, file_path), 'rb') as f:\n",
    "                data_dict = pickle.load(f)\n",
    "                if len(data_dict['data']) == 84:\n",
    "                    data.append(data_dict['data'])\n",
    "                    labels.append(class_label) # Use the numeric label\n",
    "                else:\n",
    "                    print(f\"Skipping file {file_path} due to incorrect feature count.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file_path}: {e}\")\n",
    "\n",
    "if not data or not labels:\n",
    "    print(\"No data loaded. Please run the data collection script first.\")\n",
    "    exit()\n",
    "    \n",
    "X = np.asarray(data)\n",
    "y = np.asarray(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "print(\"Training model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "y_predict = model.predict(X_test)\n",
    "score = accuracy_score(y_predict, y_test)\n",
    "print(f'{score * 100:.2f}% accuracy achieved on the test set!')\n",
    "\n",
    "# MODIFIED: Save both the model and the class map together\n",
    "save_data = {'model': model, 'class_map': class_map}\n",
    "with open('model.p', 'wb') as f:\n",
    "    pickle.dump(save_data, f)\n",
    "\n",
    "print(\"Model and class map saved as model.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e6accd-397a-4be3-be2d-c7dc6f65e3ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels loaded automatically: {0: '1', 1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'J', 11: 'K', 12: 'L', 13: 'M', 14: 'N', 15: 'O', 16: 'P', 17: 'Q', 18: 'R', 19: 'space'}\n"
     ]
    }
   ],
   "source": [
    "# 3_predict_realtime_fixed.py\n",
    "# FIXED: Now robustly handles cases with more than 2 hands detected.\n",
    "# MODIFIED: Removed the space added after each word.\n",
    "\n",
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# MODIFIED: Correctly load both the model and the class map\n",
    "try:\n",
    "    with open('model.p', 'rb') as f:\n",
    "        load_data = pickle.load(f)\n",
    "        model = load_data['model']\n",
    "        class_map = load_data['class_map']\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: model.p not found. Please run the training script first.\")\n",
    "    exit()\n",
    "\n",
    "# Automatically create the labels dictionary from the loaded class map\n",
    "labels_dict = {v: k for k, v in class_map.items()}\n",
    "print(f\"Labels loaded automatically: {labels_dict}\")\n",
    "\n",
    "# Start webcam capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize MediaPipe Hands for two hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "# Variables for the sentence formation and gesture stability logic\n",
    "sentence = \"\"\n",
    "last_prediction = None\n",
    "prediction_stable_for = 0\n",
    "last_char_time = time.time()\n",
    "PREDICTION_THRESHOLD = 20  # Number of frames a gesture must be stable\n",
    "COOLDOWN_PERIOD = 2      # Seconds to wait after adding a character\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame.\")\n",
    "        break\n",
    "    \n",
    "    H, W, _ = frame.shape\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    \n",
    "    # Create a white box at the bottom for displaying the sentence\n",
    "    cv2.rectangle(frame, (0, H - 60), (W, H), (255, 255, 255), -1)\n",
    "    \n",
    "    current_prediction = None\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        all_landmarks = []\n",
    "        # --- ⬇️ THIS IS THE MAIN FIX ⬇️ ---\n",
    "        # By slicing [:2], we guarantee this loop runs at most twice,\n",
    "        # preventing the feature count from ever exceeding 84.\n",
    "        for hand_landmarks in results.multi_hand_landmarks[:2]:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "            data_aux = []\n",
    "            x_ = [landmark.x for landmark in hand_landmarks.landmark]\n",
    "            y_ = [landmark.y for landmark in hand_landmarks.landmark]\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                data_aux.append(hand_landmarks.landmark[i].x - min(x_))\n",
    "                data_aux.append(hand_landmarks.landmark[i].y - min(y_))\n",
    "            all_landmarks.extend(data_aux)\n",
    "\n",
    "        # Pad if only one hand is detected\n",
    "        if len(all_landmarks) < 84:\n",
    "            all_landmarks.extend([0] * (84 - len(all_landmarks)))\n",
    "\n",
    "        # Make a prediction with the model\n",
    "        prediction = model.predict([np.asarray(all_landmarks)])\n",
    "        current_prediction = labels_dict[int(prediction[0])]\n",
    "\n",
    "        # Display the current predicted gesture\n",
    "        cv2.putText(frame, f'Gesture: {current_prediction}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "        \n",
    "        # Implement the delay mechanism for stable predictions\n",
    "        if current_prediction == last_prediction:\n",
    "            prediction_stable_for += 1\n",
    "        else:\n",
    "            prediction_stable_for = 1\n",
    "            last_prediction = current_prediction\n",
    "            \n",
    "        # Check if the gesture is stable and cooldown has passed\n",
    "        if prediction_stable_for >= PREDICTION_THRESHOLD and (time.time() - last_char_time) > COOLDOWN_PERIOD:\n",
    "            # --- ⬇️ THIS IS THE CHANGE ⬇️ ---\n",
    "            # Removed the \" \" to stop adding a space after each word.\n",
    "            sentence += current_prediction\n",
    "            last_char_time = time.time()\n",
    "            prediction_stable_for = 0 # Reset after adding character\n",
    "            \n",
    "    # Display the sentence being formed\n",
    "    cv2.putText(frame, sentence, (20, H - 20), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the final frame\n",
    "    cv2.imshow('Sign Language Recognition', frame)\n",
    "\n",
    "    # Allow exiting the application by pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33810b34-afb1-48dc-9e74-be0ea538a3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba034f-a237-4fac-9ef3-74ab44411535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
